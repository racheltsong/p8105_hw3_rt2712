---
title: "p8105_hw3_rt2712"
author: "Rachel Tsong"
date: "15 October 2018"
output: github_document
---

# Problem 1

## Step 1

Load and clean up the BRFSS data
```{r}
library(tidyverse)
library(p8105.datasets)

data(brfss_smart2010)

brfss_data = brfss_smart2010 %>%
  janitor::clean_names() %>%
  filter(topic == "Overall Health") %>%
  rename(state = locationabbr, county = locationdesc, proportion_responses = data_value) %>%
  select(year:county, response, proportion_responses) %>%
  mutate(response = factor(response, levels = c("Poor", "Fair", "Good", "Very good", "Excellent")))

```

## Step 2

Answer questions

**In 2002, which states were observed at 7 locations?**
```{r}
brfss_data %>%
  filter(year == 2002) %>%
  distinct(county, .keep_all = TRUE) %>%
  group_by(state) %>%
  summarize(n = n()) %>%
  filter(n == 7)%>%
  as_tibble()

```

In 2002, there were 3 states that were observed at exactly 7 locations (Connecticut, Florida, and North Carolina).

**“Spaghetti plot” showing the number of locations in each state from 2002 to 2010**
```{r}
library(ggplot2)

brfss_data %>%
  group_by(year, state) %>%
  summarize(n_counties = n_distinct(county)) %>%
  ggplot(aes(x = year, y = n_counties, color = state)) + 
<<<<<<< HEAD
    labs(x = "Year", y = "Number of Counties", color = "State", title = "Number of Counties Surveyed by BRFSS by State (2002 - 2010)") +
    geom_line(size = 1)

```

The spaghetti plot above shows the number of different counties that were surveyed by the BRFSS from 2002 to 2010 in each state. Since all 50 states and DC are represented in the data set, it is hard to distinguish between the different states in the plot by color. For example, in 2007 and 2010 one state had significantly more locations surveyed, but it is hard to tell which state this is (Delaware? Florida?). However, I couldn't really figure out a better way to distinguish between the states.

**Mean and standard deviation of the proportion of “Excellent” responses across locations in NY State (2002, 2006, and 2010)**
=======
  labs(x = "Year", y = "Number of Counties", color = "State") +
  geom_line(size = 1)

```

The spaghetti plot above shows the number of different counties that were surveyed by the BRFSS from 2002 to 2010 in each state. Since all 50 states and DC are representd in the data set, it is hard to distinguish between the different states in the plot by color. However, there isn't really a better way to distinguish between the states, so while the figure looks pretty nice, it is not necessarily a good figure to draw conclusions from.

**Table showing mean and standard deviation of the proportion of “Excellent” responses across locations in NY State (2002, 2006, and 2010)**

>>>>>>> 92e8a716bebb98a634dc27d0d9dc3fdd30c2a00c
```{r}
brfss_data %>%
  filter(state == "NY", response == "Excellent", year == 2002 | year == 2006 | year == 2010) %>%
  group_by(year) %>%
  summarize(mean_excellent = mean(proportion_responses), sd_excellent = sd(proportion_responses)) %>%
<<<<<<< HEAD
  knitr::kable(digits = 1)

```

From the summary table above, it appears as if the mean number of respondents who consider themselves to be in "Excellent" health was about the same in 2002, 2006, and 2010 (about 22-24% of respondents). Looking at the standard deviations, it appears as if there was the highest variation in the proportion of excellent responses by county in NY in 2002 and the lowest variation in 2010. 

**Five-panel plot showing the distribution of state-level averages over time for each response category**
```{r}
brfss_data %>%
  group_by(year, state, response) %>%
  summarize(mean = mean(proportion_responses)) %>% 
  ggplot(aes(x = year, y = mean)) + 
    geom_point() + 
    facet_grid(~response) +
    labs(x = "Year", y = "Average Proportion of Responses", title = "Average Proportion of Responses Over Time") +
    theme(axis.text.x = element_text(angle = 90))
  
```

In this 5 panel plot, we can see how the proportion of responses for each state varies across time. Also, since I plotted a point for each state, we can get an idea of how varied the data are within each category from year to year. Overall, there is not a lot of variation in mean response from year to year, but there is more variation based on location.

# Problem 2

## Step 1

```{r}
data("instacart")

head(instacart)
  
```

**Dataset description**  
This dataset contains information about customer orders from Instacart, a grocery delivery company. First, there is some information about the customer including the customer ID, days since last Instacart order, number of times they've ordered from Instacart, and the day and time the customer placed the order. The day is only given as the day of the week numbered from 0 to 6, and I assumed 0 was Sunday, though this is unclear. The time ordered is given as a single value, which I assume to be hours since midnight (e.g. "13" means 1:00 pm). An interesting variable is the "reordered" column. This column in the dataset is characterized as an integer vector, but since it only takes on values of 0 and 1, I am thinking it is most likely a logical vector indicating whether or not the customer has ordered that same item in a previous order. Additionally, there is information about the contents of the order including the product name and where the product can be found in the store (department and aisle). For example, the product "Bulgarian yogurt" is identified by a numerical product ID, and it is in the "yogurt" aisle in the "dairy & eggs" department. The aisle and deparment variables are also identified by numerical IDs. There are 1,384,617 observations of 15 variables. 

## Step 2

Answer questions

**How many aisles are there, and which aisles are the most items ordered from**
```{r}
instacart %>% 
  group_by(aisle) %>%
  summarize(n = n()) %>%
  arrange(-n) %>%
  head() %>%
  knitr::kable()
```

There are 134 aisles, and the most ordered from aisles are shown in the table above.

**Plot showing the number of items ordered in each aisle**
```{r fig.width = 10, fig.height = 10}
instacart %>%
  group_by(department, aisle, aisle_id) %>%
  filter(aisle != "missing") %>%
  summarize(n = n()) %>%
  ungroup() %>%
  mutate(aisle = factor(aisle, levels = aisle[order(n)])) %>%
  ggplot(aes(x = aisle, y = n, color = department, fill = department)) +
    geom_bar(stat = "identity", width = 0.5) + 
    coord_flip() +
    theme(axis.text.y = element_text(size = rel(0.6)), legend.position = "bottom" ) +
    labs(x = "Number of Orders", y = "Aisle", title = "Number of Orders on Instacart by Aisle and Department", legend = "Department")
 
```

In the plot above, I chose to order the aisle by the number of orders. Even after discarding the ambiguous "missing" aisle, there are 133 aisles, so it was a challenge figuring how best to display all of the aisles in a readable way. One option that would have increased readability was to display the aisles by their ID number, but since this would require one to reference another data set in order to know which aisle matched which ID, I really wanted to display the aisle names. Flipping the coordinate axes helped with this problem. Additionally, I chose to use colors to display the department each aisle belonged to so that the figure shows more information - for example, it is interesting to note that the top 3 most ordered from aisles are all from the produce department.

**Most popular item in “baking ingredients”, “dog food care”, and “packaged vegetables fruits”**
```{r}
instacart %>% 
  group_by(aisle, product_name) %>%
  filter(aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits") %>%
  summarize(n = n()) %>%
  filter(n == max(n)) %>%
  knitr::kable()
  
```

The most popular items from each aisle are listed above. "Packaged vegetables fruits" is one of the most ordered from aisles, so it is unsurprising that the most ordered item within that aisle is ordered much more often than the most ordered items in the other two aisles.

**Table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week**
```{r}
instacart %>%
  mutate(order_dow = factor(order_dow, 0:6, c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))) %>%
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>%
  group_by(order_dow, product_name) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  spread(key = order_dow, value = mean_hour) %>%
  knitr::kable(digits = 1)
```

Generally, it seems as is people are buying coffee ice cream and pink lady apples at about the same mean time on Sunday, Wednesday, and Friday. On the other days the mean hour that coffee ice cream is ordered is a couple of hours after the mean hour that pink lady apples are ordered.

# Problem 3

## Step 1

Load NY NOAA data and describe the data set
```{r}
data("ny_noaa")

head(ny_noaa)
```

Some exploration into the data
```{r eval = FALSE}
sum(is.na(ny_noaa$prcp))
sum(is.na(ny_noaa$snow))
sum(is.na(ny_noaa$snwd))
sum(is.na(ny_noaa$tmax))
sum(is.na(ny_noaa$tmin))
range(ny_noaa$date)
n_distinct(ny_noaa$id)
```


**Description**  
This data set contains information about weather in New York from 747 NOAA weather stations from January 1, 1981 to December 31, 2010. Key variables are the high and low temperatures for the day and precipitation information, including cumulative snow depth.  Although this data set is quite large with almost 2.6 million observations for 7 variables, there is a lot of missing data. For example, there are about 1.13 million missing observations for maximum and minimum temperature. These missing values restrict what inferences can be drawn from the data set. 

## Step 2

Clean the data and answer questions

**Create separate variables for year, month, and day and ensure observations have reasonable units**
```{r}
ny_noaa_tidy = separate(ny_noaa, date, into = c("year", "month", "day"), sep = "-") %>%
  mutate(year = as.numeric(year), 
         month = as.numeric(month), 
         day = as.numeric(day), 
         tmax_degrees = as.numeric(tmax)/10, 
         tmin_degrees = as.numeric(tmin)/10,
         prcp_mm = prcp/10,
         snow_mm = snow/10,
         snwd_mm = snwd/10) %>%
  select(-(prcp:tmin)) 

head(ny_noaa_tidy)
```

In the code above, I separated the data into columns for year, month, and day and transformed variables into numerical vectors. I spent a lot of time confused about the units for precipitation and temperature, and after Google, as well as some discussion forum insight, it seems as if the units are in tenths of a millimeter for precipitation and tenths of a degree (Celsius) for temperature. While these units might make sense when we are interested in small changes such as rainfall per minute, it does not make sense to report data for an entire day in these units, so I changed the units to millimeters and degrees Celsius. Additionally, it seems as if some of the temperature readings are inaccurate. Part of the reason I was struggling to figure out the units on temperature in particular is because the max temperature (even after correcting for tenths of a degree) is 60 degrees Celsius for 34 observations. A cursory web search indicating that the maximum recorded air temperature on Earth is only 57.6 degrees implies some sort of instrument or data input error. The fact that these observations are clustered together for a single station in August and September of 2008 supports this hypothesis.

**What are the most commonly observed values for snowfall and why?**
```{r}
ny_noaa_tidy %>%
  group_by(snow_mm) %>%
  summarize(n=n()) %>%
  arrange(-n) %>%
  head() %>%
  knitr::kable()
```

The most commonly observed values for snowfall are 0 and NA. Obviously, most days in New York experience no snowfall. As previously discussed, the data set contains a large number of missing values; it is unclear whether this is due to neglect or some sort of instrument malfunction. The other top values are 1.3, 2.5, 5.1, and 7.6 centimeters. Though this collection of numbers seemed random at first, once I recognized that there are about 2.54 centimeters per inch, it made more sense why these were the most commonly observed values.

**Two-panel plot showing the average max temperature in January and July in each station across years**
```{r}
ny_noaa_tidy %>%
  filter(month == 1 | month == 7) %>%
  mutate(month = month.name[month]) %>%
  drop_na() %>%
  group_by(month, year, id) %>%
  summarize(mean_tmax = mean(tmax_degrees)) %>% 
  ggplot(aes(x = year, y = mean_tmax)) +
    geom_point() +
    facet_grid(~month) +
    labs(x = "Year", y = "Average Maximum Temperature (Celsius)", title = "New York Maximum Temperatures in Summer and Winter") 

```

**Is there any observable / interpretable structure? Any outliers?**
Looking at these two graphs, there are a couple of interesting features to note. First, the data for January max temperatures within a single year are more spread out than the data for July max temperatures, meaning different stations record more varied temperature data in January and more consistent temperature data in July. From year to year, the variation in January is also greater than the variation in July: July is consistently hot every year, but January has some relatively warmer years and some exceptionally colder years. Regarding outliers, there are more noticable outliers in January. It appears as if 1982 and 1996 have a station recording a particularly cold January and 1997 and 2004 have a station that recorded a warmer than average January. In July there is one lucky area in 1988 that recorded a brisk average max temperature in July of 14 degrees - either a very strange year or a faulty temperature gauge.

**Two-panel plot showing:**  
*(i) tmax vs tmin for the full dataset,  
*(ii) distribution of snowfall values > 0 and < 100 separately by year
```{r fig.width = 6}
library(hexbin)
library(viridis)

temp_plot = ny_noaa_tidy %>%
  select(tmax_degrees, tmin_degrees) %>%
  drop_na() %>%
  ggplot(aes(x = tmin_degrees, y = tmax_degrees, colour = ..count..)) +
  geom_hex() +
  scale_fill_viridis() + 
  scale_color_viridis() +
  labs(x = "Minimum temperature (Celsius)", y = "Maximum temperature (Celsius)", title = "Density Plot of Daily High and Low Temperatures") 

snow_plot = ny_noaa_tidy %>%
  select(snow_mm, year) %>%
  drop_na %>%
  filter(snow_mm > 0 & snow_mm < 10) %>%
  ggplot(aes(x = year, y = snow_mm, group = year)) +
  geom_violin() +
  labs(x = "Year", y = "Snowfall (mm)", title = "Distribution of Snowfall Across Time")

library(gridExtra)

grid.arrange(temp_plot, snow_plot, nrow = 2, top = "New York Temperature and Snowfall Data")
```

The plots above show New York temperature and snowfall data. In the temperature plot, average temperature minima and maxima are positively correlated as expected. The yellow/green areas show that most of the time, these temperatures are highly linearly correlated. Rarer temperature events are in the dark blue areas of the plot. Many of the outliers are more likely errors instead of instances of extreme temperatures.

The snowfall data plot shows that the amount snowfall each year in New York is consistent from year to year. I chose to display these data as a violin plot to note an interesting feature: the bulges in the graph at 1.3, 2.5, 5.0, and 7.5 mm. As noted before, these values correspond to 1/2, 1, 2, and 3 inches. Therefore, it can be concluded that when reporting the amount of snowfall, the data collectors rounded to the nearest inch for snowfall $\geq 1$ inch and the nearest 1/2 inch for snowfall $<1$ inch.
=======
  knitr::kable()

```


>>>>>>> 92e8a716bebb98a634dc27d0d9dc3fdd30c2a00c



